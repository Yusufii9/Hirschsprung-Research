{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100ccc21-b447-4a62-bb1c-a58a2f2e9fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import og_mae\n",
    "from youssef_plexus_data_loading import HirschImagesDataset\n",
    "from metrics import mean_iou\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625106f2-6279-4689-a622-98fdc13a644b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('actual_plexus_saved_models/ViT_IN1k_plexus_0.0002.pt')\n",
    "backbone_state_dict = checkpoint['backbone']\n",
    "linear_state_dict = checkpoint['linear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b42305b6-898f-4660-b7c5-7a450acb511e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_iou(y_pred, y_true):\n",
    "    smooth = 0.0001\n",
    "    # ytrue, ypred is a flatten vector\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_true = y_true.flatten()\n",
    "    current = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    # compute mean iou\n",
    "    intersection = np.diag(current)\n",
    "    ground_truth_set = current.sum(axis=1)\n",
    "    predicted_set = current.sum(axis=0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = (intersection+smooth) / (union.astype(np.float32)+smooth)\n",
    "    return np.mean(IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddba55f7-4767-425a-b1ca-683dcad84a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mean_inclusion(y_pred, y_true):\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_true = y_true.flatten()\n",
    "    \n",
    "    current = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    print(current)\n",
    "    \n",
    "    ground_truth_area = current[1, 1] + current[1, 0]\n",
    "    print(ground_truth_area)\n",
    "    \n",
    "    included_area = current[1, 1]\n",
    "    print(included_area)\n",
    "    \n",
    "    inclusion_percentage = (included_area / ground_truth_area) * 100 if ground_truth_area > 0 else 0\n",
    "    \n",
    "    return inclusion_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4317bd8b-c9de-46c2-96be-df49285904e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.model = og_mae.mae_vit_base_patch16_dec512d8b().cuda()\n",
    "        self.linear = nn.Linear(768, 512).cuda()\n",
    "\n",
    "    def forward_features(self, img):\n",
    "        x = self.model.patch_embed(img)\n",
    "        x = x + self.model.pos_embed[:, 1:, :]\n",
    "\n",
    "        cls_token = self.model.cls_token + self.model.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.model.blocks:\n",
    "            x = blk(x)  # (bsz, L, 768)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.linear(x)  # (bsz, L, 512)\n",
    "        return x\n",
    "\n",
    "original_model = OriginalModel()\n",
    "original_model.model.load_state_dict(backbone_state_dict)\n",
    "original_model.linear.load_state_dict(linear_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b4f43d5-6164-4c4a-8ed8-1cf30ffc6b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, compute_mean_inclusion, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    thresh = 0.5\n",
    "    all_predictions_test = []\n",
    "    all_gt_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            img, plexus = batch  # load from batch\n",
    "            img = img.cuda().to(dtype=torch.bfloat16) / 255  # (bsz, 3, H, W)\n",
    "            plexus = plexus.cuda().long().squeeze(dim=1)  # (bsz, H, W)\n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                x = model.forward_features(img)\n",
    "                x = model.linear(x)  # (bsz, L, 512)\n",
    "                logits = rearrange(x[:, 1:, :], 'b (h w) (c i j) -> b c (h i) (w j)', h=14, w=14, c=2, i=16, j=16)  # (bsz, 2, H, W)\n",
    "                probability = logits.softmax(dim=1)\n",
    "                predictions = (probability[:, 1, :, :] > thresh).long()\n",
    "\n",
    "            all_predictions_test.append(predictions.cpu())\n",
    "            all_gt_test.append(plexus.cpu())\n",
    "\n",
    "        all_predictions_test = torch.cat(all_predictions_test, dim=0).numpy()\n",
    "        all_gt_test = torch.cat(all_gt_test, dim=0).numpy()\n",
    "\n",
    "        test_miou = compute_mean_inclusion(all_predictions_test, all_gt_test)\n",
    "    \n",
    "    return test_miou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1995b349-3001-4bf1-8c68-92d6bad90104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[350159699    171798]\n",
      " [   309420    591083]]\n",
      "900503\n",
      "591083\n",
      "[[350205166    308706]\n",
      " [   238973    479155]]\n",
      "718128\n",
      "479155\n",
      "Val Mean Inclusion: 65.63920\n",
      "Test Mean Inclusion: 66.72278\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "val_dataset = HirschImagesDataset(data_file_path=\"plexus_val\", do_augmentation=False)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=8\n",
    "                       )\n",
    "\n",
    "test_dataset = HirschImagesDataset(data_file_path=\"plexus_test\", do_augmentation=False)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=8\n",
    "                        )\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "original_model.to(device)\n",
    "\n",
    "val_miou = evaluate_model(original_model, val_loader, compute_mean_inclusion, device)\n",
    "test_miou = evaluate_model(original_model, test_loader, compute_mean_inclusion, device)\n",
    "print(f'Val Mean Inclusion: {val_miou:.5f}')\n",
    "print(f'Test Mean Inclusion: {test_miou:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee2630-a95e-4df9-b828-4667534fddff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
